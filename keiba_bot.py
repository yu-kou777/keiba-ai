# -*- coding: utf-8 -*-
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3
from bs4 import BeautifulSoup
import pandas as pd
import sys
import re
import time
import random

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- è¨­å®šï¼šDiscord Webhook ---
DISCORD_URL = "https://discordapp.com/api/webhooks/1473026116825645210/9eR_UIp-YtDqgKem9q4cD9L2wXrqWZspPaDhTLB6HjRQyLZU-gaUCKvKbf2grX7msal3"

LAB_PLACE_MAP = {"æœ­å¹Œ":"01","å‡½é¤¨":"02","ç¦å³¶":"03","æ–°æ½Ÿ":"04","æ±äº¬":"05","ä¸­å±±":"06","ä¸­äº¬":"07","äº¬éƒ½":"08","é˜ªç¥":"09","å°å€‰":"10"}

def get_stealth_headers():
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15"
    ]
    return {"User-Agent": random.choice(user_agents), "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8", "Referer": "https://www.google.com/"}

def analyze_horse_history(session, horse_url, name):
    """
    ã€é¦¬æŸ±ã‚¹ã‚­ãƒ£ãƒ³æ©Ÿèƒ½ã€‘
    å„é¦¬ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸ã¸é£›ã³ã€éå»5èµ°ã®ã‚¿ã‚¤ãƒ å·®ã‚’æŠ½å‡ºã€‚
    """
    try:
        # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã¸å¤‰æ›
        if not horse_url.startswith("http"):
            full_url = "https://www.keibalab.jp" + horse_url
        else:
            full_url = horse_url
            
        print(f"  ğŸ” {name} ã®é¦¬æŸ±(éå»5èµ°)ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...") # è¨¼æ‹ ã‚’è¡¨ç¤º
        time.sleep(random.uniform(0.8, 1.5))
        
        res = session.get(full_url, headers=get_stealth_headers(), timeout=15, verify=False)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # éå»æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
        rows = soup.select('table.db-horse-table tbody tr')
        if not rows:
            print(f"    âš ï¸ {name}: éå»ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 0, False, "ãƒ‡ãƒ¼ã‚¿ãªã—"
        
        diffs = []
        for row in rows[:5]: # ç›´è¿‘5èµ°ã‚’æŠ½å‡º
            tds = row.find_all('td')
            if len(tds) < 14: continue
            
            # ã‚¿ã‚¤ãƒ å·®(ç€å·®)ã®ã‚«ãƒ©ãƒ ã‚’æ¢ã™
            val = 99.9
            for td in tds:
                txt = td.text.strip()
                if re.match(r'^\(?\-?\d+\.\d+\)?$', txt):
                    val = float(re.sub(r'[^\d\.-]', '', txt))
                    break
            
            if val < 5.0:
                diffs.append(val)

        if not diffs: return 0, False, "å¯¾è±¡èµ°ãªã—"
        
        # --- ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ ---
        score = 0
        best_diff = min(diffs)
        if best_diff <= 0.0: score += 60      
        elif best_diff <= 0.3: score += 45    
        regret_count = sum(1 for d in diffs if 0.0 <= d <= 0.5)
        score += regret_count * 15            
        
        return score, best_diff
    except Exception as e:
        return 0, 99.9

def get_future_race(d, p, r):
    """
    ã€æœªæ¥äºˆæ¸¬ãƒ»ç‰¹åŒ–å‹ã€‘
    shutsubahyou.html ã‚’å¼·åˆ¶çš„ã«è¦‹ã«è¡Œãã€æœªæ¥ã®å‡ºèµ°é¦¬ã‚’æ•æ‰ã€‚
    """
    if not d: d = time.strftime("%Y%m%d")
    p_code = LAB_PLACE_MAP.get(p, "05")
    # æœªæ¥ã®å‡ºé¦¬è¡¨URLã‚’ç”Ÿæˆ
    url = f"https://www.keibalab.jp/db/race/{d}{p_code}{str(r).zfill(2)}/shutsubahyou.html"
    
    print(f"ğŸ“¡ æœªæ¥äºˆæ¸¬ãƒ•ã‚§ãƒ¼ã‚ºå§‹å‹•: {url}")
    session = requests.Session()
    session.mount("https://", HTTPAdapter(max_retries=3))
    
    try:
        res = session.get(url, headers=get_stealth_headers(), timeout=30, verify=False)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ãƒ¬ãƒ¼ã‚¹å
        title_tag = soup.select_one('h1.raceTitle')
        title = title_tag.text.strip().replace('\n', ' ') if title_tag else "æœªæ¥ã®ãƒ¬ãƒ¼ã‚¹"
        
        horses = []
        # å‡ºé¦¬è¡¨ã‹ã‚‰é¦¬ã®æƒ…å ±ã‚’æŠ½å‡º
        rows = soup.find_all('tr')
        print(f"ğŸ“Š ã€Œ{title}ã€ã®å‡ºèµ°é¦¬ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        
        for row in rows:
            # é¦¬ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            link_tag = row.select_one('a[href*="/db/horse/"]')
            if not link_tag: continue
            
            name = link_tag.text.strip()
            # é¦¬ç•ª
            tds = row.find_all('td')
            umaban = "0"
            for i, td in enumerate(tds):
                if td == link_tag.find_parent('td'):
                    if i > 0 and tds[i-1].text.strip().isdigit(): umaban = tds[i-1].text.strip()
                    break
            
            # ã‚ªãƒƒã‚ºï¼ˆæœªæ¥ã®å ´åˆã€ã¾ã å‡ºã¦ã„ãªã„å¯èƒ½æ€§ã‚’è€ƒæ…®ï¼‰
            odds_m = re.search(r'(\d{1,4}\.\d{1})', row.text)
            odds = float(odds_m.group(1)) if odds_m else 99.9
            
            # --- ã“ã“ã§ã€Œé¦¬ã®å±¥æ­´ï¼ˆé¦¬æŸ±ï¼‰ã€ã‚’è¦‹ã«è¡Œã ---
            score, best = analyze_horse_history(session, link_tag.get('href'), name)
            
            is_chaos = (best <= 0.6 and odds > 15.0)
            horses.append({"num": int(umaban), "name": name, "score": score, "is_ana": is_chaos, "best": best})
            
        return horses, title
    except Exception as e:
        print(f"âŒ è§£æå¤±æ•—: {e}")
        return [], "Err"

def send_result(horses, title, d, p, r):
    if not horses or len(horses) < 3: return
    df = pd.DataFrame(horses).sort_values('score', ascending=False).reset_index(drop=True)
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ­ã‚¸ãƒƒã‚¯
    top_score = df.iloc[0]['score']
    gap = top_score - df.iloc[min(3, len(df)-1)]['score']
    limit = 5 if top_score >= 75 and gap >= 15 else 9
    
    row1 = df.head(3)['num'].tolist()
    row2 = df.head(5)['num'].tolist()
    ana = df[df['is_ana']]['num'].tolist()
    row3 = list(dict.fromkeys(row2 + ana + df.iloc[5:8]['num'].tolist()))[:limit]

    payload = {
        "username": "æ•™æˆAI (æœªæ¥è¦³æ¸¬ãƒ¢ãƒ¼ãƒ‰) ğŸ‡",
        "embeds": [{
            "title": f"ğŸ¯ ã€æœªæ¥äºˆæ¸¬ã€‘{p}{r}R {title}",
            "description": f"ğŸ“… {d} | **3-5-{limit} æ§‹æˆ**",
            "color": 3066993,
            "fields": [
                {"name": "ğŸ“Š è§£æãƒ­ã‚°", "value": f"```å…¨é ­ã®éå»5èµ°(é¦¬æŸ±)ã‚’ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†ã€‚\nè»¸ä¿¡é ¼åº¦: {top_score} / åˆ¤å®š: {'ç²¾å¯†' if limit==5 else 'åºƒåŸŸ'}```", "inline": False},
                {"name": "ğŸ”¥ è»¸ãƒ»ç›¸æ‰‹", "value": f"**{', '.join(map(str, row1))}** â†’ **{', '.join(map(str, row2))}**", "inline": False},
                {"name": "ğŸ’° æ¨å¥¨ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³", "value": f"1åˆ—ç›®: {row1}\n2åˆ—ç›®: {row2}\n3åˆ—ç›®: {row3}", "inline": False}
            ]
        }]
    }
    requests.post(DISCORD_URL, json=payload)
    print(f"âœ… Discordã¸æœªæ¥ã®äºˆæƒ³ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    args = sys.argv
    date = args[1] if len(args) > 1 else "" # æŒ‡å®šãŒãªã‘ã‚Œã°ä»Šæ—¥
    place = args[2] if len(args) > 2 else "æ±äº¬"
    race = args[3] if len(args) > 3 else "11"
    
    h, t = get_future_race(date, place, race)
    send_result(h, t, date, place, race)
    input("\nè§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚Enterã‚­ãƒ¼ã§é–‰ã˜ã¾ã™...")
