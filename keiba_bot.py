import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import sys
import re
import json

# ==========================================
# âš™ï¸ è¨­å®šï¼šDiscord Webhook URL
# ==========================================
# ğŸ‘‡ Discordã®URLã¯ãã®ã¾ã¾æ®‹ã—ã¦ãã ã•ã„
DISCORD_WEBHOOK_URL = "https://discordapp.com/api/webhooks/1473026116825645210/9eR_UIp-YtDqgKem9q4cD9L2wXrqWZspPaDhTLB6HjRQyLZU-gaUCKvKbf2grX7msal3"

PLACE_MAP = {
    "æœ­å¹Œ": "01", "å‡½é¤¨": "02", "ç¦å³¶": "03", "æ–°æ½Ÿ": "04",
    "æ±äº¬": "05", "ä¸­å±±": "06", "ä¸­äº¬": "07", "äº¬éƒ½": "08",
    "é˜ªç¥": "09", "å°å€‰": "10"
}

def find_race_id(date_str, place_name, race_num):
    y = date_str[:4]
    p = PLACE_MAP.get(place_name, "05")
    r = str(race_num).zfill(2)
    try:
        m = int(date_str[4:6])
        d = int(date_str[6:8])
        target_date_text = f"{m}æœˆ{d}æ—¥"
    except:
        return None

    print(f"ğŸ” '{target_date_text}' ã® {place_name} {race_num}R ã‚’æœç´¢ä¸­...")

    for kai in range(1, 7):
        for day in range(1, 13):
            race_id = f"{y}{p}{str(kai).zfill(2)}{str(day).zfill(2)}{r}"
            url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                res = requests.get(url, headers=headers, timeout=5)
                res.encoding = 'EUC-JP'
                html = res.text
                if target_date_text in html and ("å‡ºé¦¬è¡¨" in html or "ãƒ¬ãƒ¼ã‚¹çµæœ" in html):
                    print(f"âœ… ç™ºè¦‹: {race_id}")
                    return race_id
            except:
                continue
    return None

def get_data(race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    res.encoding = 'EUC-JP'
    soup = BeautifulSoup(res.text, 'html.parser')

    # ãƒ¬ãƒ¼ã‚¹åå–å¾—ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    race_name = "ãƒ¬ãƒ¼ã‚¹åä¸æ˜"
    # å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«
    r_name_div = soup.find('div', class_='RaceName')
    if r_name_div:
        race_name = r_name_div.text.strip()
    else:
        # çµæœãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«(h1ãªã©)
        h1_title = soup.find('h1', class_='RaceName')
        if h1_title:
            race_name = h1_title.text.strip()
        else:
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨æ¸¬
            title_tag = soup.find('title')
            if title_tag:
                race_name = title_tag.text.split('ï½œ')[0]

    # é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    horses = []
    rows = soup.select('tr.HorseList')
    if not rows:
        rows = soup.select('table.RaceTable01 tr') # çµæœãƒšãƒ¼ã‚¸ç”¨

    for row in rows:
        try:
            umaban_tag = row.select_one('td.Umaban') or row.select_one('td:nth-of-type(1)')
            name_tag = row.select_one('span.HorseName') or row.select_one('a[href*="horse"]')
            if not umaban_tag or not name_tag: continue

            umaban = umaban_tag.text.strip()
            if not umaban.isdigit(): continue
            name = name_tag.text.strip()
            
            # ã‚ªãƒƒã‚ºå–å¾—
            odds = 99.9
            # äººæ°—ã‚¿ã‚°ãŒã‚ã‚Œã°ãã“ã‹ã‚‰ã‚ªãƒƒã‚ºæ¨æ¸¬ï¼ˆç°¡æ˜“ï¼‰
            pop_tag = row.select_one('span.Popular')
            odds_tag = row.select_one('td.Odds')
            
            if odds_tag:
                txt = odds_tag.text.strip()
                if txt.replace('.','').isdigit():
                    odds = float(txt)
            
            # ã‚†ãƒ¼ã“ã†ãƒ­ã‚¸ãƒƒã‚¯ç°¡æ˜“ç‰ˆ
            score = 0
            if odds < 30: score += (100 / odds)
            
            jockey_tag = row.select_one('td.Jockey')
            if jockey_tag:
                jockey = jockey_tag.text.strip()
                if any(x in jockey for x in ['ãƒ«ãƒ¡', 'å·ç”°', 'æ­¦è±Š', 'å‚äº•', 'æˆ¸å´', 'ãƒ¬ãƒ¼ãƒ³']):
                    score += 15
            
            horses.append({"é¦¬ç•ª": umaban, "é¦¬å": name, "ã‚ªãƒƒã‚º": odds, "ã‚¹ã‚³ã‚¢": score})
        except:
            continue

    if not horses: return None, race_name
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
    df = pd.DataFrame(horses)
    df = df.sort_values('ã‚¹ã‚³ã‚¢', ascending=False)
    return df.head(6).to_dict('records'), race_name

def send_discord(ranks, race_name, date_str, place, r_num):
    if "http" not in DISCORD_WEBHOOK_URL:
        print("âš ï¸ Discord URLæœªè¨­å®š")
        return

    honmei = ranks[0]
    taikou = ranks[1]
    tana = ranks[2]
    
    # ã‚ªãƒƒã‚ºãŒ99.9ï¼ˆå–å¾—å¤±æ•—ï¼‰ã®å ´åˆã¯è¡¨ç¤ºã‚’å¤‰ãˆã‚‹
    odds_str = f"{honmei['ã‚ªãƒƒã‚º']}" if honmei['ã‚ªãƒƒã‚º'] != 99.9 else "å–å¾—ä¸å¯(çµ‚äº†ãƒ¬ãƒ¼ã‚¹)"

    msg = {
        "username": "ã‚†ãƒ¼ã“ã†AI ğŸ‡",
        "embeds": [{
            "title": f"ğŸ¯ AIäºˆæƒ³: {place}{r_num}R {race_name}",
            "description": f"ğŸ“… {date_str} | ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯è§£æ",
            "color": 16776960,
            "fields": [
                {"name": "â— æœ¬å‘½", "value": f"**{honmei['é¦¬ç•ª']} {honmei['é¦¬å']}**\n(ã‚ªãƒƒã‚º: {odds_str})", "inline": True},
                {"name": "ã€‡ å¯¾æŠ—", "value": f"**{taikou['é¦¬ç•ª']} {taikou['é¦¬å']}**", "inline": True},
                {"name": "â–² å˜ç©´", "value": f"**{tana['é¦¬ç•ª']} {tana['é¦¬å']}**", "inline": True},
                {"name": "æ¨å¥¨è²·ã„ç›® (3é€£å˜F)", "value": f"1ç€: {honmei['é¦¬ç•ª']}\n2ç€: {taikou['é¦¬ç•ª']}, {tana['é¦¬ç•ª']}\n3ç€: æµã— ({ranks[3]['é¦¬ç•ª']}...)", "inline": False}
            ]
        }]
    }
    requests.post(DISCORD_WEBHOOK_URL, json=msg)

if __name__ == "__main__":
    if len(sys.argv) > 3:
        d, p, r = sys.argv[1], sys.argv[2], sys.argv[3]
    else:
        d, p, r = "20260214", "æ±äº¬", "11"

    print(f"ğŸš€ {d} {p} {r}R è§£æé–‹å§‹")
    rid = find_race_id(d, p, r)
    if rid:
        data, name = get_data(rid)
        if data:
            send_discord(data, name, d, p, r)
            print("âœ… å®Œäº†")
        else:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãªã—")
    else:
        print("âŒ IDç‰¹å®šå¤±æ•—")
