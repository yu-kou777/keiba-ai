import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3
from bs4 import BeautifulSoup
import pandas as pd
import sys
import re
import time
import random

# SSLè­¦å‘Šã‚’ç„¡è¦–ï¼ˆçªç ´åŠ›å‘ä¸Šã®ãŸã‚ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- Discordæ¥ç¶šè¨­å®š ---
DISCORD_URL = "https://discordapp.com/api/webhooks/1473026116825645210/9eR_UIp-YtDqgKem9q4cD9L2wXrqWZspPaDhTLB6HjRQyLZU-gaUCKvKbf2grX7msal3"

LAB_PLACE_MAP = {"æœ­å¹Œ":"01","å‡½é¤¨":"02","ç¦å³¶":"03","æ–°æ½Ÿ":"04","æ±äº¬":"05","ä¸­å±±":"06","ä¸­äº¬":"07","äº¬éƒ½":"08","é˜ªç¥":"09","å°å€‰":"10"}

# ç©¶æ¥µã®å½è£…ãƒ˜ãƒƒãƒ€ãƒ¼ç¾¤
def get_stealth_headers():
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8", # æ—¥æœ¬èªç’°å¢ƒã‚’ä¸»å¼µ
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://www.google.com/", # Googleæ¤œç´¢ã‹ã‚‰æ¥ãŸãƒ•ãƒªã‚’ã™ã‚‹
        "Connection": "keep-alive",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "cross-site",
        "Upgrade-Insecure-Requests": "1"
    }

def create_session():
    session = requests.Session()
    retries = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session

def safe_float(value):
    try:
        clean = re.sub(r'[^\d\.-]', '', str(value))
        return float(clean)
    except: return 99.9

def analyze_singularity(session, horse_url, odds):
    """è©³ç´°è§£æ"""
    try:
        if not horse_url.startswith("http"):
            horse_url = "https://www.keibalab.jp" + horse_url
            
        time.sleep(random.uniform(1.5, 3.0)) # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
        
        # verify=Falseã§SSLã®å³å¯†ãªãƒã‚§ãƒƒã‚¯ã‚’å›é¿
        res = session.get(horse_url, headers=get_stealth_headers(), timeout=30, verify=False)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table.db-horse-table tbody tr')
        
        if not rows: return 0, False, "ãƒ‡ãƒ¼ã‚¿ãªã—"
        
        diffs = []
        for row in rows[:3]:
            tds = row.find_all('td')
            if len(tds) < 14: continue
            
            found_diff = False
            for td in tds:
                txt = td.text.strip()
                if re.match(r'^\(?\-?\d+\.\d+\)?$', txt):
                    val = safe_float(txt)
                    if val < 5.0:
                        diffs.append(val)
                        found_diff = True
                        break
            
            if not found_diff and len(tds) > 11:
                if "1" in tds[11].text.strip(): diffs.append(0.0)

        if not diffs: return 0, False, "ã‚¿ã‚¤ãƒ å·®ä¸æ˜"
        
        score = sum(60 for d in diffs if d <= 0.3)
        avg_diff = sum(diffs) / len(diffs)
        score += max(0, 1.5 - avg_diff) * 20
        is_chaos = (avg_diff <= 0.8 and odds > 15.0)
        
        return score, is_chaos, f"å¹³å‡å·®:{avg_diff:.2f}"
    except Exception:
        return 0, False, "è§£æã‚¨ãƒ©ãƒ¼"

def get_race_data(date_str, place_name, race_num):
    if not date_str or len(date_str) < 8:
        date_str, place_name, race_num = "20260207", "äº¬éƒ½", "11"

    p_code = LAB_PLACE_MAP.get(place_name, "08")
    r_num = str(race_num).zfill(2)
    url = f"https://www.keibalab.jp/db/race/{date_str}{p_code}{r_num}/"
    
    print(f"ğŸ“¡ æ½œå…¥é–‹å§‹: {url}")
    session = create_session()
    
    try:
        # verify=False ã‚’è¿½åŠ 
        res = session.get(url, headers=get_stealth_headers(), timeout=30, verify=False)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        title_tag = soup.select_one('h1.raceTitle')
        title = title_tag.text.strip().replace('\n', ' ') if title_tag else "ãƒ¬ãƒ¼ã‚¹æƒ…å ±"
        print(f"ğŸ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•æ‰: {title}")
        
        horses = []
        rows = soup.find_all('tr')
        
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œå‹•...")
        for row in rows:
            name_tag = row.select_one('a[href*="/db/horse/"]')
            if not name_tag: continue
            
            try:
                name = name_tag.text.strip()
                umaban = "0"
                tds = row.find_all('td')
                for i, td in enumerate(tds):
                    if td == name_tag.find_parent('td'):
                        if i > 0:
                            prev = tds[i-1].text.strip()
                            if prev.isdigit(): umaban = prev
                        break
                
                if umaban == "0": continue

                odds = 99.9
                m = re.search(r'(\d{1,4}\.\d{1})', row.text)
                if m: odds = float(m.group(1))
                
                j_tag = row.select_one('a[href*="/db/jockey/"]')
                jockey = j_tag.text.strip() if j_tag else ""

                score, is_chaos, note = analyze_singularity(session, name_tag.get('href'), odds)
                
                if any(x in jockey for x in ['ãƒ«ãƒ¡', 'å·ç”°', 'æ­¦è±Š', 'å‚äº•', 'æˆ¸å´']):
                    score += 15
                
                print(f"  âˆš {umaban}ç•ª {name}: Score {score:.1f}")
                
                horses.append({
                    "num": int(umaban),
                    "name": name,
                    "score": score,
                    "is_ana": is_chaos,
                    "odds": odds
                })
            except: continue
                
        return horses, title
    except Exception as e:
        print(f"âŒ æ¥ç¶šé®æ–­: {e}")
        return [], "ã‚¨ãƒ©ãƒ¼"

def send_to_discord(horses, title, d, p, r):
    if not horses:
        print("âŒ è§£æãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆIPãƒ–ãƒ­ãƒƒã‚¯ã®å¯èƒ½æ€§å¤§ï¼‰")
        return

    df = pd.DataFrame(horses).sort_values('score', ascending=False).reset_index(drop=True)
    
    axis = df.head(2)['num'].tolist()
    row2 = df.head(4)['num'].tolist()
    
    ana_list = df[df['is_ana']]['num'].tolist()
    candidates = row2 + ana_list + df.iloc[4:8]['num'].tolist()
    row3 = list(dict.fromkeys(candidates))[:6]

    buy_str = (
        f"**1ç€**: {', '.join(map(str, axis))}\n"
        f"**2ç€**: {', '.join(map(str, row2))}\n"
        f"**3ç€**: {', '.join(map(str, row3))}"
    )
    
    payload = {
        "username": "æ•™æˆAI (ã‚¹ãƒ†ãƒ«ã‚¹ãƒ¢ãƒ¼ãƒ‰) ğŸ‡",
        "embeds": [{
            "title": f"ğŸ¯ {p}{r}R {title}",
            "description": f"ğŸ“… {d} | **24ç‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³**",
            "color": 3066993,
            "fields": [
                {"name": "ğŸ‘‘ 1ç€è»¸", "value": f"**{', '.join(map(str, axis))}**", "inline": True},
                {"name": "ğŸ 2ç€å€™è£œ", "value": f"{', '.join(map(str, row2))}", "inline": True},
                {"name": "ğŸŒ€ 3ç€å€™è£œ", "value": f"{', '.join(map(str, row3))}", "inline": False},
                {"name": "ğŸ’° æ¨å¥¨è²·ã„ç›® (24ç‚¹)", "value": buy_str, "inline": False}
            ]
        }]
    }
    
    requests.post(DISCORD_URL, json=payload)
    print("âœ… Discordé€ä¿¡å®Œäº†")

if __name__ == "__main__":
    try:
        args = sys.argv
        date = args[1] if len(args) > 1 else "20260207"
        place = args[2] if len(args) > 2 else "äº¬éƒ½"
        race = args[3] if len(args) > 3 else "11"
    except:
        date, place, race = "20260207", "äº¬éƒ½", "11"
    
    h_list, t_str = get_race_data(date, place, race)
    send_to_discord(h_list, t_str, date, place, race)
