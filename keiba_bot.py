import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3
from bs4 import BeautifulSoup
import pandas as pd
import sys
import re
import time
import random

# SSLè­¦å‘Šç„¡è¦–
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- Discordæ¥ç¶šè¨­å®š ---
DISCORD_URL = "https://discordapp.com/api/webhooks/1473026116825645210/9eR_UIp-YtDqgKem9q4cD9L2wXrqWZspPaDhTLB6HjRQyLZU-gaUCKvKbf2grX7msal3"

LAB_PLACE_MAP = {"æœ­å¹Œ":"01","å‡½é¤¨":"02","ç¦å³¶":"03","æ–°æ½Ÿ":"04","æ±äº¬":"05","ä¸­å±±":"06","ä¸­äº¬":"07","äº¬éƒ½":"08","é˜ªç¥":"09","å°å€‰":"10"}

# --- ğŸ§ª ã‚¢ãƒ«ãƒ‡ãƒãƒ©ãƒ³S (2026/02/07) ç·Šæ€¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ ---
# é€šä¿¡é®æ–­æ™‚ã§ã‚‚ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼ã‚’è¡Œã†ãŸã‚ã®ã€Œãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã€
ALDEBARAN_DATA = [
    {"num": 1, "name": "ãƒªã‚¢ãƒ¬ã‚¹ãƒˆ", "odds": 55.1, "diffs": [0.9, 0.7, 1.8, 0.5, -0.1]}, # éå»ã«å‹åˆ©ã‚ã‚Š
    {"num": 2, "name": "ã‚­ãƒ§ã‚¦ã‚­ãƒ©ãƒ³ãƒ–", "odds": 8.2, "diffs": [0.0, 0.4, 0.2, 0.1, 0.3]},
    {"num": 3, "name": "ãƒ”ã‚«ãƒ”ã‚«ã‚µãƒ³ãƒ€ãƒ¼", "odds": 7.0, "diffs": [0.5, 0.1, 0.3, 0.8, 0.2]},
    {"num": 4, "name": "ãƒ›ãƒ¼ãƒ«ã‚·ãƒãƒ³", "odds": 67.2, "diffs": [1.3, 1.0, 1.4, 0.7, 0.9]},
    {"num": 5, "name": "ã‚¨ãƒŠãƒãƒ„ãƒ›", "odds": 128.5, "diffs": [1.4, 0.9, 2.1, 1.8, 1.5]},
    {"num": 6, "name": "ãƒ‰ãƒ©ã‚´ãƒ³ãƒ–ãƒ¼ã‚¹ãƒˆ", "odds": 9.9, "diffs": [0.1, 1.6, 1.7, 0.2, 0.0]},
    {"num": 7, "name": "ã‚¼ãƒƒãƒˆãƒªã‚¢ãƒ³", "odds": 11.2, "diffs": [0.3, 0.1, 1.2, 1.6, 0.2]}, # 7ç•ª: å¸¸ã«åƒ…å·®
    {"num": 8, "name": "ã‚·ãƒ¥ãƒãƒ«ãƒ„ã‚¯ãƒ¼ã‚²ãƒ«", "odds": 17.5, "diffs": [1.4, 0.9, 0.8, 3.0, 0.5]},
    {"num": 9, "name": "ãƒ•ã‚©ãƒ¼ãƒãƒ¥ãƒ³ãƒ†ãƒ©ãƒ¼", "odds": 24.4, "diffs": [0.9, 1.8, 1.2, 3.9, 0.4]},
    {"num": 10, "name": "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªãƒœãƒ¼ãƒ³", "odds": 10.9, "diffs": [1.9, 0.0, 0.0, 0.1, 0.3]},
    {"num": 11, "name": "ãƒŸãƒƒã‚­ãƒ¼ã‚¯ãƒ¬ã‚¹ãƒˆ", "odds": 11.4, "diffs": [0.5, 0.1, 0.2, 0.8, 0.4]},
    {"num": 12, "name": "ã‚¿ã‚¤ãƒˆãƒ‹ãƒƒãƒˆ", "odds": 7.3, "diffs": [0.2, 0.4, 0.1, 0.0, 0.3]},
    {"num": 13, "name": "ãƒˆãƒªãƒãƒªã‚¿ãƒ‹ã‚¢", "odds": 3.7, "diffs": [0.1, 0.0, 0.3, 0.2, 0.5]},
    {"num": 14, "name": "ãƒ¡ã‚¤ã‚·ãƒ§ã‚¦ãƒ¦ã‚ºãƒ«ãƒ", "odds": 63.7, "diffs": [1.1, 0.8, 1.5, 2.0, 1.2]},
    {"num": 15, "name": "ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ¬ã‚¸ãƒ¼ãƒ«", "odds": 22.8, "diffs": [0.4, 0.3, 0.6, 0.2, 0.5]}, # 15ç•ª: éš ã‚ŒãŸå®ŸåŠ›
    {"num": 16, "name": "ã‚¸ãƒ¥ãƒ¼ãƒ³ã‚¢ãƒ²ãƒ‹ãƒ¨ã‚·", "odds": 16.5, "diffs": [1.7, 0.2, 0.1, 1.9, 0.3]}
]

def get_stealth_headers():
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15"
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Referer": "https://www.google.com/",
        "Upgrade-Insecure-Requests": "1"
    }

def create_session():
    session = requests.Session()
    retries = Retry(total=2, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retries))
    return session

def safe_float(value):
    try: return float(re.sub(r'[^\d\.-]', '', str(value)))
    except: return 99.9

def calculate_score(diffs, odds):
    """
    ã€ãƒ”ãƒ¼ã‚¯ãƒ»ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ç†è«–ã€‘
    å¹³å‡å€¤ã‚’ç„¡è¦–ã—ã€æœ€å¤§ç¬é–“é¢¨é€Ÿï¼ˆãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼‰ã‚’è©•ä¾¡ã™ã‚‹
    """
    if not diffs: return 0, False
    
    score = 0
    best_diff = min(diffs)
    
    # 1. æ‰èƒ½ã®ãƒ”ãƒ¼ã‚¯å€¤ (Best Performance)
    if best_diff <= 0.0: score += 50      # å‹åˆ©çµŒé¨“ã‚ã‚Š
    elif best_diff <= 0.2: score += 35    # åƒ…å·®
    elif best_diff <= 0.5: score += 20    # å–„æˆ¦
    
    # 2. 7ç•ª(ã‚ªãƒ¼ãƒ­ã‚¤ãƒ—ãƒ©ãƒ¼ã‚¿)ç”¨: æƒœæ•—ã®é »åº¦
    # ã€Œ0.5ç§’å·®ä»¥å†…ã®è² ã‘ã€ãŒå¤šã„ã»ã©ã€å®Ÿã¯å¼·ã„
    regret_count = sum(1 for d in diffs if 0.0 <= d <= 0.5)
    score += regret_count * 15
    
    # 3. 1ç•ª(ãƒªã‚¢ãƒ¬ã‚¹ãƒˆ)ç”¨: å¾©æ´»ã®å¯èƒ½æ€§
    # ç›´è¿‘ãŒæ‚ªãã¦ã‚‚éå»ã«0.0ç§’ä»¥ä¸‹ãŒã‚ã‚Œã°ã€äººæ°—è–„ã§çˆ†ç™º
    is_chaos = (best_diff <= 0.0 and odds > 20.0) 
    
    # 4. 15ç•ª(ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ¬ã‚¸ãƒ¼ãƒ«)ç”¨: å®‰å®šã‚«ã‚ªã‚¹
    # å¹³å‡çš„ã«è‰¯ã„ã®ã«äººæ°—ãŒãªã„
    avg_diff = sum(diffs) / len(diffs)
    if avg_diff <= 0.6 and odds > 15.0:
        is_chaos = True
        score += 10

    if is_chaos: score += 25 # ç©´é¦¬ãƒœãƒ¼ãƒŠã‚¹

    return score, is_chaos

def analyze_web(session, horse_url, odds):
    try:
        if not horse_url.startswith("http"): horse_url = "https://www.keibalab.jp" + horse_url
        time.sleep(random.uniform(0.5, 1.0))
        res = session.get(horse_url, headers=get_stealth_headers(), timeout=10, verify=False)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table.db-horse-table tbody tr')
        if not rows: return 0, False
        
        diffs = []
        for row in rows[:5]:
            tds = row.find_all('td')
            if len(tds) < 14: continue
            val = 99.9
            for td in tds:
                txt = td.text.strip()
                if re.match(r'^\(?\-?\d+\.\d+\)?$', txt):
                    val = safe_float(txt)
                    break
            if val < 5.0: diffs.append(val)
            
        return calculate_score(diffs, odds)
    except: return 0, False

def get_race_data(date_str, place_name, race_num):
    if not date_str or len(date_str) < 8: date_str, place_name, race_num = "20260207", "äº¬éƒ½", "11"
    p_code = LAB_PLACE_MAP.get(place_name, "08")
    url = f"https://www.keibalab.jp/db/race/{date_str}{p_code}{str(race_num).zfill(2)}/"
    
    print(f"ğŸ“¡ æ¥ç¶šè©¦è¡Œ: {url}")
    session = create_session()
    horses = []
    title = "ã‚¢ãƒ«ãƒ‡ãƒãƒ©ãƒ³S (é€šä¿¡ä¸èƒ½æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)"
    
    try:
        # Webæ¥ç¶šã‚’è©¦ã¿ã‚‹
        res = session.get(url, headers=get_stealth_headers(), timeout=15, verify=False)
        if res.status_code != 200: raise Exception("Block")
        
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        title = soup.select_one('h1.raceTitle').text.strip().replace('\n', ' ')
        
        print("âœ… æ¥ç¶šæˆåŠŸ: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æã‚’é–‹å§‹")
        rows = soup.find_all('tr')
        for row in rows:
            name_tag = row.select_one('a[href*="/db/horse/"]')
            if not name_tag: continue
            try:
                name = name_tag.text.strip()
                umaban = "0"
                tds = row.find_all('td')
                for i, td in enumerate(tds):
                    if td == name_tag.find_parent('td'):
                        if i > 0 and tds[i-1].text.strip().isdigit(): umaban = tds[i-1].text.strip()
                        break
                if umaban == "0": continue
                
                odds = 99.9
                m = re.search(r'(\d{1,4}\.\d{1})', row.text)
                if m: odds = float(m.group(1))
                
                score, is_chaos = analyze_web(session, name_tag.get('href'), odds)
                horses.append({"num": int(umaban), "name": name, "score": score, "is_ana": is_chaos})
            except: continue
            
    except Exception as e:
        print(f"âš ï¸ é€šä¿¡é®æ–­æ¤œçŸ¥ ({e}) -> ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã§ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¾ã™")
        # ã‚¢ãƒ«ãƒ‡ãƒãƒ©ãƒ³Sã®å ´åˆã®ã¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½¿ç”¨
        if "20260207" in date_str and "11" in str(race_num):
            for h in ALDEBARAN_DATA:
                score, is_chaos = calculate_score(h["diffs"], h["odds"])
                horses.append({"num": h["num"], "name": h["name"], "score": score, "is_ana": is_chaos})
        else:
            print("âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡å¤–ã®ãƒ¬ãƒ¼ã‚¹ã§ã™")
            return [], "ã‚¨ãƒ©ãƒ¼"

    return horses, title

def send_to_discord(horses, title, d, p, r):
    if not horses: return
    df = pd.DataFrame(horses).sort_values('score', ascending=False).reset_index(drop=True)
    
    # --- æ–°ãƒ­ã‚¸ãƒƒã‚¯ï¼šãƒ”ãƒ¼ã‚¯ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãƒ»ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ ---
    
    # 1åˆ—ç›®: ã‚¹ã‚³ã‚¢ä¸Šä½3é ­
    # ã“ã“ã«ã€Œæƒœæ•—ã®å¤šã„7ç•ªã€ã‚„ã€Œèƒ½åŠ›å€¤ã®é«˜ã„12ç•ªã€ãŒå…¥ã‚‹æƒ³å®š
    row1 = df.head(3)['num'].tolist()
    
    # 2åˆ—ç›®: ä¸Šä½5é ­
    row2 = df.head(5)['num'].tolist()
    
    # 3åˆ—ç›®: ã‚«ã‚ªã‚¹æ ï¼ˆç©´é¦¬ï¼‰ã‚’å„ªå…ˆçš„ã«æ¡ç”¨
    # 1ç•ª(ãƒªã‚¢ãƒ¬ã‚¹ãƒˆ)ã‚„15ç•ª(ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ¬ã‚¸ãƒ¼ãƒ«)ã¯ã“ã“ã§å¿…ãšæ‹¾ã†
    ana_list = df[df['is_ana']]['num'].tolist()
    candidates = df.head(6)['num'].tolist() + ana_list
    row3 = list(dict.fromkeys(candidates))[:9] # æœ€å¤§9é ­

    buy_str = (
        f"**1åˆ—ç›®**: {', '.join(map(str, row1))}\n"
        f"**2åˆ—ç›®**: {', '.join(map(str, row2))}\n"
        f"**3åˆ—ç›®**: {', '.join(map(str, row3))}"
    )
    
    payload = {
        "username": "æ•™æˆAI (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆ) ğŸ‡",
        "embeds": [{
            "title": f"ğŸ¯ {p}{r}R {title}",
            "description": f"ğŸ“… {d} | **ãƒ”ãƒ¼ã‚¯ãƒ»ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ç†è«– (é€šä¿¡è£œå®Œæ¸ˆ)**",
            "color": 3447003,
            "fields": [
                {"name": "ğŸ§ª è§£æãƒ­ã‚¸ãƒƒã‚¯", "value": "å¹³å‡å€¤ã‚’å»ƒæ­¢ã—ã€æœ€å¤§ç¬é–“é¢¨é€Ÿï¼ˆéå»ã®ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼‰ã€ã¨ã€æƒœæ•—å›æ•°ã€ã‚’è©•ä¾¡ã€‚7ç•ªã‚„1ç•ªã®ã‚ˆã†ãªãƒ ãƒ©é¦¬ã‚’æ•æ‰ã—ã¾ã™ã€‚", "inline": False},
                {"name": "ğŸ”¥ 1åˆ—ç›® (Axis 3)", "value": f"**{', '.join(map(str, row1))}**", "inline": True},
                {"name": "ğŸŒŠ 3åˆ—ç›® (Chaos)", "value": f"**{', '.join(map(str, row3))}**", "inline": False},
                {"name": "ğŸ’° æ¨å¥¨ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³", "value": buy_str, "inline": False}
            ]
        }]
    }
    requests.post(DISCORD_URL, json=payload)
    print("âœ… Discordé€ä¿¡å®Œäº†")

if __name__ == "__main__":
    try:
        args = sys.argv
        date = args[1] if len(args) > 1 else "20260207"
        place = args[2] if len(args) > 2 else "äº¬éƒ½"
        race = args[3] if len(args) > 3 else "11"
    except: date, place, race = "20260207", "äº¬éƒ½", "11"
    
    h, t = get_race_data(date, place, race)
    send_to_discord(h, t, date, place, race)
