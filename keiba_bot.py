import requests
from bs4 import BeautifulSoup
import pandas as pd
import sys
import re
import time

# --- è¨­å®šï¼šDiscord Webhook URL ---
DISCORD_URL = "https://discordapp.com/api/webhooks/1473026116825645210/9eR_UIp-YtDqgKem9q4cD9L2wXrqWZspPaDhTLB6HjRQyLZU-gaUCKvKbf2grX7msal3"

LAB_PLACE_MAP = {"æœ­å¹Œ":"01","å‡½é¤¨":"02","ç¦å³¶":"03","æ–°æ½Ÿ":"04","æ±äº¬":"05","ä¸­å±±":"06","ä¸­äº¬":"07","äº¬éƒ½":"08","é˜ªç¥ž":"09","å°å€‰":"10"}

def get_performance_details(horse_url):
    """éŽåŽ»3èµ°ã®ã‚¿ã‚¤ãƒ å·®ã¨å®‰å®šæ€§ã‚’åˆ†æžã™ã‚‹"""
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        time.sleep(0.4)
        res = requests.get("https://www.keibalab.jp" + horse_url, headers=headers, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table.db-horse-table tbody tr')
        
        diffs = []
        stability_count = 0 # 0.5ç§’ä»¥å†…ã«å…¥ã£ãŸå›žæ•°
        
        for row in rows[:3]:
            tds = row.find_all('td')
            if len(tds) > 13:
                txt = tds[13].text.strip()
                match = re.search(r'(-?\d+\.\d+)', txt)
                if match:
                    d = float(match.group(1))
                    diffs.append(d)
                    if d <= 0.5: stability_count += 1
        
        if not diffs: return 0, 0
        
        # ã‚¿ã‚¤ãƒ å·®ã‚¹ã‚³ã‚¢
        weights = [1.0, 0.7, 0.4]
        t_score = sum(max(0, 1.2 - d) * 15 * weights[i] for i, d in enumerate(diffs))
        
        # å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹ï¼ˆ7ç•ªã‚„15ç•ªã‚’æ‹¾ã†ãŸã‚ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        s_bonus = stability_count * 20
        
        return t_score, s_bonus
    except: return 0, 0

def get_lab_data(date_str, place_name, race_num):
    p_code = LAB_PLACE_MAP.get(place_name, "05")
    r_num = str(race_num).zfill(2)
    base_url = f"https://www.keibalab.jp/db/race/{date_str}{p_code}{r_num}/"
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        print(f"ðŸš€ ã€7ç•ªè»¸ãƒ­ã‚¸ãƒƒã‚¯ã€‘å®‰å®šæ€§ï¼‹ã‚¿ã‚¤ãƒ å·®åˆ†æžã‚’é–‹å§‹...")
        res = requests.get(base_url, headers=headers, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        title = soup.select_one('h1.raceTitle').text.strip().replace('\n', ' ') if soup.select_one('h1.raceTitle') else "è§£æž"
        
        horses, seen_num = [], set()
        rows = soup.find_all('tr')
        
        for row in rows:
            name_tag = row.select_one('a[href*="/db/horse/"]')
            if not name_tag or len(row.find_all('td')) < 5: continue
            
            try:
                name = name_tag.text.strip()
                horse_url = name_tag.get('href')
                
                # é¦¬ç•ªç‰¹å®š
                td_list = row.find_all('td')
                umaban = ""
                for td in td_list:
                    if td.text.strip().isdigit() and 1 <= int(td.text.strip()) <= 18:
                        umaban = td.text.strip()
                        if td.find_next_sibling() and td.find_next_sibling().select_one('a[href*="/db/horse/"]'): break
                
                if not umaban or umaban in seen_num: continue
                seen_num.add(umaban)

                jockey = row.select_one('a[href*="/db/jockey/"]').text.strip() if row.select_one('a[href*="/db/jockey/"]') else "ä¸æ˜Ž"
                odds = float(re.search(r'(\d{1,4}\.\d{1})', row.text).group(1)) if re.search(r'(\d{1,4}\.\d{1})', row.text) else 999.0

                # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æžå®Ÿè¡Œ
                t_score, s_bonus = get_performance_details(horse_url)
                
                # é¨Žæ‰‹è£œæ­£
                j_bonus = 15 if any(x in jockey for x in ['ãƒ«ãƒ¡', 'å·ç”°', 'æ­¦è±Š', 'å‚äº•', 'æˆ¸å´Ž', 'å²©ç”°æœ›', 'é®«å³¶']) else 0
                
                total_score = t_score + s_bonus + j_bonus
                
                horses.append({"num": int(umaban), "name": name, "jockey": jockey, "odds": odds, "score": total_score})
                print(f"  ðŸ” {umaban}ç•ª {name}: ã‚¹ã‚³ã‚¢ç®—å‡ºå®Œäº†")
            except: continue
            
        return horses, title
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"); return [], "ã‚¨ãƒ©ãƒ¼"

def send_discord(horses, title, d, p, r):
    if not horses: return
    df = pd.DataFrame(horses).sort_values('score', ascending=False).reset_index(drop=True)
    
    # ðŸŽ¯ 2è»¸ã®é¸å®šï¼ˆã‚¹ã‚³ã‚¢1ä½ã¨2ä½ï¼‰
    axis = df.head(2)
    axis_nums = axis['num'].tolist()
    
    # ðŸŽ ç›¸æ‰‹å€™è£œï¼ˆç´ï¼š3ä½ã€œ8ä½ï¼‰
    opponents = df.iloc[2:8]['num'].tolist()
    
    payload = {
        "username": "ã‚†ãƒ¼ã“ã†AI (é ­2è»¸ãƒ»100ä¸‡ç‹™ã„) ðŸ‡",
        "embeds": [{
            "title": f"ðŸŽ¯ {p}{r}R {title}",
            "description": f"ðŸ“… {d} | **ã€3é€£å˜ é ­2è»¸ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã€‘**",
            "color": 15548997,
            "fields": [
                {"name": "ðŸ‘‘ 1ç€å›ºå®š(2è»¸)", "value": f"**{axis_nums[0]}ç•ª** ({axis.iloc[0]['name']})\n**{axis_nums[1]}ç•ª** ({axis.iloc[1]['name']})", "inline": False},
                {"name": "ðŸŽ ç›¸æ‰‹ (2ãƒ»3ç€å€™è£œ)", "value": f"{', '.join(map(str, opponents))}", "inline": False},
                {"name": "ðŸ’° 3é€£å˜ æŽ¨å¥¨", "value": f"**1ç€**: {axis_nums[0]}, {axis_nums[1]}\n**2ç€**: {axis_nums[0]}, {axis_nums[1]}, {opponents[0]}, {opponents[1]}\n**3ç€**: å…¨æµã— (é«˜é…å½“ç‹™ã„)", "inline": False},
                {"name": "ðŸ“ˆ ãƒ­ã‚¸ãƒƒã‚¯è§£èª¬", "value": "ç›´è¿‘3èµ°ã§0.5ç§’ä»¥å†…ã®å®‰å®šã—ãŸèµ°ã‚Šã‚’è¦‹ã›ã¦ã„ã‚‹é¦¬ã‚’è»¸ã«æ®ãˆã¾ã—ãŸã€‚15ç•ªã®ã‚ˆã†ãªç©´é¦¬ã®æ¿€èµ°ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ã€‚", "inline": False}
            ]
        }]
    }
    requests.post(DISCORD_URL, json=payload)

if __name__ == "__main__":
    args = sys.argv
    date = args[1] if len(args) > 1 and args[1] != "" else "20260222"
    place = args[2] if len(args) > 2 and args[2] != "" else "æ±äº¬"
    race = args[3] if len(args) > 3 else "11"
    h, t = get_lab_data(date, place, race)
    send_discord(h, t, date, place, race)
