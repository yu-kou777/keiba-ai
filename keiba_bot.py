import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pandas as pd
import sys
import re
import time
import random

# --- Discordæ¥ç¶šè¨­å®š ---
DISCORD_URL = "https://discordapp.com/api/webhooks/1473026116825645210/9eR_UIp-YtDqgKem9q4cD9L2wXrqWZspPaDhTLB6HjRQyLZU-gaUCKvKbf2grX7msal3"

LAB_PLACE_MAP = {"æœ­å¹Œ":"01","å‡½é¤¨":"02","ç¦å³¶":"03","æ–°æ½Ÿ":"04","æ±äº¬":"05","ä¸­å±±":"06","ä¸­äº¬":"07","äº¬éƒ½":"08","é˜ªç¥":"09","å°å€‰":"10"}

# å½è£…ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒªã‚¹ãƒˆ
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0"
]

def create_session():
    """æ¥ç¶šã‚’ç¶­æŒã—ã€åˆ‡æ–­ã•ã‚Œã¦ã‚‚é£Ÿã‚‰ã„ã¤ãã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    session = requests.Session()
    retries = Retry(
        total=5,  # 5å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤
        backoff_factor=2,  # å¾…æ©Ÿæ™‚é–“ã‚’å€ã€…ã«å¢—ã‚„ã™ (2ç§’, 4ç§’, 8ç§’...)
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session

def safe_float(value):
    try:
        clean = re.sub(r'[^\d\.-]', '', str(value))
        return float(clean)
    except: return 99.9

def analyze_singularity(session, horse_url, odds):
    """éå»3èµ°è§£æï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³å¼•ãç¶™ãç‰ˆï¼‰"""
    headers = {"User-Agent": random.choice(USER_AGENTS)}
    try:
        if not horse_url.startswith("http"):
            horse_url = "https://www.keibalab.jp" + horse_url
            
        # ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
        time.sleep(random.uniform(1.0, 2.0))
        
        res = session.get(horse_url, headers=headers, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table.db-horse-table tbody tr')
        
        if not rows: return 0, False, "ãƒ‡ãƒ¼ã‚¿ãªã—"
        
        diffs = []
        for row in rows[:3]:
            tds = row.find_all('td')
            if len(tds) < 14: continue
            
            # ã‚¿ã‚¤ãƒ å·®æŠ½å‡º
            found_diff = False
            for td in tds:
                txt = td.text.strip()
                if re.match(r'^\(?\-?\d+\.\d+\)?$', txt):
                    val = safe_float(txt)
                    if val < 5.0:
                        diffs.append(val)
                        found_diff = True
                        break
            
            # ç€é †ã‹ã‚‰è£œå®Œ
            if not found_diff and len(tds) > 11:
                if "1" in tds[11].text.strip(): diffs.append(0.0)

        if not diffs: return 0, False, "ã‚¿ã‚¤ãƒ å·®ä¸æ˜"
        
        # ç‰©ç†ã‚¹ã‚³ã‚¢è¨ˆç®—
        score = sum(60 for d in diffs if d <= 0.3)
        avg_diff = sum(diffs) / len(diffs)
        score += max(0, 1.5 - avg_diff) * 20
        is_chaos = (avg_diff <= 0.8 and odds > 15.0)
        
        return score, is_chaos, f"å¹³å‡å·®:{avg_diff:.2f}"
    except Exception:
        return 0, False, "è§£æã‚¨ãƒ©ãƒ¼"

def get_race_data(date_str, place_name, race_num):
    if not date_str or len(date_str) < 8:
        date_str, place_name, race_num = "20260207", "äº¬éƒ½", "11"

    p_code = LAB_PLACE_MAP.get(place_name, "08")
    r_num = str(race_num).zfill(2)
    url = f"https://www.keibalab.jp/db/race/{date_str}{p_code}{r_num}/"
    
    print(f"ğŸ“¡ è¦³æ¸¬é–‹å§‹: {url}")
    session = create_session()
    headers = {"User-Agent": random.choice(USER_AGENTS)}
    
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒ¬ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸å–å¾—
        res = session.get(url, headers=headers, timeout=30)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        t_elem = soup.select_one('h1.raceTitle')
        title = t_elem.text.strip().replace('\n', ' ') if t_elem else "ãƒ¬ãƒ¼ã‚¹åä¸æ˜"
        print(f"ğŸ å¯¾è±¡: {title}")
        
        horses = []
        rows = soup.find_all('tr')
        
        print("ğŸ” å…¨é ­ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹...")
        for row in rows:
            name_tag = row.select_one('a[href*="/db/horse/"]')
            if not name_tag: continue
            
            try:
                name = name_tag.text.strip()
                
                # é¦¬ç•ªå–å¾—
                umaban = "0"
                tds = row.find_all('td')
                for i, td in enumerate(tds):
                    if td == name_tag.find_parent('td'):
                        if i > 0:
                            prev = tds[i-1].text.strip()
                            if prev.isdigit(): umaban = prev
                        break
                
                if umaban == "0": continue

                # ã‚ªãƒƒã‚º
                odds = 99.9
                m = re.search(r'(\d{1,4}\.\d{1})', row.text)
                if m: odds = float(m.group(1))
                
                # é¨æ‰‹
                j_tag = row.select_one('a[href*="/db/jockey/"]')
                jockey = j_tag.text.strip() if j_tag else ""

                # è©³ç´°è§£æï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ¸¡ã™ï¼‰
                score, is_chaos, note = analyze_singularity(session, name_tag.get('href'), odds)
                
                # é¨æ‰‹è£œæ­£
                if any(x in jockey for x in ['ãƒ«ãƒ¡', 'å·ç”°', 'æ­¦è±Š', 'å‚äº•', 'æˆ¸å´']):
                    score += 15
                
                print(f"  âˆš {umaban}ç•ª {name}: Score {score:.1f}")
                
                horses.append({
                    "num": int(umaban),
                    "name": name,
                    "score": score,
                    "is_ana": is_chaos,
                    "odds": odds
                })
            except: continue
                
        return horses, title
    except Exception as e:
        print(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return [], "ã‚¨ãƒ©ãƒ¼"

def send_to_discord(horses, title, d, p, r):
    if not horses:
        print("âŒ è§£æãƒ‡ãƒ¼ã‚¿ãªã—")
        return

    df = pd.DataFrame(horses).sort_values('score', ascending=False).reset_index(drop=True)
    
    # 24ç‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
    axis = df.head(2)['num'].tolist()
    row2 = df.head(4)['num'].tolist()
    
    ana_list = df[df['is_ana']]['num'].tolist()
    candidates = row2 + ana_list + df.iloc[4:8]['num'].tolist()
    row3 = list(dict.fromkeys(candidates))[:6]

    buy_str = (
        f"**1ç€**: {', '.join(map(str, axis))}\n"
        f"**2ç€**: {', '.join(map(str, row2))}\n"
        f"**3ç€**: {', '.join(map(str, row3))}"
    )
    
    payload = {
        "username": "æ•™æˆAI (å†æ¥ç¶šæˆåŠŸ) ğŸ‡",
        "embeds": [{
            "title": f"ğŸ¯ {p}{r}R {title}",
            "description": f"ğŸ“… {d} | **é€šä¿¡éšœå®³çªç ´ãƒ»è§£æå®Œäº†**",
            "color": 3066993,
            "fields": [
                {"name": "ğŸ‘‘ 1ç€è»¸", "value": f"**{', '.join(map(str, axis))}**", "inline": True},
                {"name": "ğŸ 2ç€å€™è£œ", "value": f"{', '.join(map(str, row2))}", "inline": True},
                {"name": "ğŸŒ€ 3ç€å€™è£œ", "value": f"{', '.join(map(str, row3))}", "inline": False},
                {"name": "ğŸ’° æ¨å¥¨è²·ã„ç›® (24ç‚¹)", "value": buy_str, "inline": False}
            ]
        }]
    }
    
    requests.post(DISCORD_URL, json=payload)
    print("âœ… Discordé€ä¿¡å®Œäº†")

if __name__ == "__main__":
    try:
        args = sys.argv
        date = args[1] if len(args) > 1 else "20260207"
        place = args[2] if len(args) > 2 else "äº¬éƒ½"
        race = args[3] if len(args) > 3 else "11"
    except:
        date, place, race = "20260207", "äº¬éƒ½", "11"
    
    h_list, t_str = get_race_data(date, place, race)
    send_to_discord(h_list, t_str, date, place, race)
