import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import random

# --- å¯¾ç­–â‘ : ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã•ã‚‰ã«è©³ç´°åŒ– (iPhone 15ãƒ—ãƒ­ä»•æ§˜) ---
HEADERS = {
    "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ja-jp",
    "Connection": "keep-alive",
}

# --- å¯¾ç­–â‘¡: @st.cache_data ã‚’ä½¿ã£ã¦ã€ŒåŒã˜ãƒ‡ãƒ¼ã‚¿ã¯äºŒåº¦å–ã‚‰ãªã„ã€ ---
@st.cache_data(ttl=3600) # 1æ™‚é–“ã¯ãƒãƒƒãƒˆã‚’è¦‹ã«è¡Œã‹ãšã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†
def get_html_safe(url):
    time.sleep(random.uniform(2.0, 5.0)) # äººé–“ãŒç”»é¢ã‚’çœºã‚ã‚‹æ™‚é–“ã‚’å½è£…
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code == 200:
            response.encoding = response.apparent_encoding
            return response.text
        else:
            st.error(f"ã‚µã‚¤ãƒˆãŒæ··ã¿åˆã£ã¦ã„ã¾ã™ (Status: {response.status_code})")
            return None
    except Exception as e:
        st.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- å¯¾ç­–â‘¢: ç¨®ç‰¡é¦¬ãƒ‡ãƒ¼ã‚¿ãŒå–ã‚Œãªã„æ™‚ã®ãŸã‚ã®ã€Œå›ºå®šãƒªã‚¹ãƒˆã€ ---
# ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µã‚¤ãƒˆãŒè½ã¡ã¦ã„ã¦ã‚‚ã‚¢ãƒ—ãƒªãŒå‹•ã‹ãªããªã‚‹ã®ã‚’é˜²ãã¾ã™
DEFAULT_SIRES = ["ã‚­ã‚ºãƒŠ", "ãƒ­ãƒ¼ãƒ‰ã‚«ãƒŠãƒ­ã‚¢", "ã‚¨ãƒ”ãƒ•ã‚¡ãƒã‚¤ã‚¢", "ãƒ‰ã‚¥ãƒ©ãƒ¡ãƒ³ãƒ†", "ãƒ¢ãƒ¼ãƒªã‚¹", "ãƒãƒ¼ãƒ„ã‚¯ãƒ©ã‚¤", "ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ"]

@st.cache_data
def get_sire_rankings():
    url = "https://db.netkeiba.com/?pid=sire_leading"
    html = get_html_safe(url)
    if not html: return DEFAULT_SIRES
    
    soup = BeautifulSoup(html, "html.parser")
    rows = soup.select(".nk_tb_common tr")
    sires = [row.find_all("td")[1].text.strip() for row in rows[1:51] if len(row.find_all("td")) > 1]
    return sires if sires else DEFAULT_SIRES

# --- ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ---
st.title("ğŸ‡ é‰„å£ç‰ˆãƒ»AIç«¶é¦¬äºˆæƒ³")

with st.sidebar:
    st.write("ğŸ“¡ æ¥ç¶šã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: æ­£å¸¸")
    if st.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"):
        st.cache_data.clear()
        st.success("å†å–å¾—ã®æº–å‚™ãŒã§ãã¾ã—ãŸ")

# IDå…¥åŠ›éƒ¨åˆ†
race_id = st.text_input("ãƒ¬ãƒ¼ã‚¹ID", "202602070811")

if st.button("æ…é‡ã«åˆ†æã‚’é–‹å§‹"):
    with st.spinner("ã‚µã‚¤ãƒˆã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã€ã‚†ã£ãã‚Šè§£æã—ã¦ã„ã¾ã™..."):
        # ç¨®ç‰¡é¦¬å–å¾—
        top_sires = get_sire_rankings()
        
        # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
        url_lab = f"https://www.keibalab.jp/db/race/{race_id}/"
        html_lab = get_html_safe(url_lab)
        
        if html_lab:
            soup = BeautifulSoup(html_lab, "html.parser")
            # é¦¬åã¨çˆ¶åã‚’æŠœãï¼ˆæ­£è¦è¡¨ç¾ã‚’ä½¿ã‚ãšç¢ºå®Ÿã«ã‚¿ã‚°ã§æŒ‡å®šï¼‰
            rows = soup.select(".table_01 tr")[1:]
            results = []
            for row in rows:
                tds = row.find_all("td")
                if len(tds) > 12:
                    name = tds[3].text.strip()
                    sire = tds[4].text.split('\n')[0].strip()
                    odds = tds[12].text.strip()
                    results.append({"é¦¬å": name, "çˆ¶": sire, "ã‚ªãƒƒã‚º": odds})
            
            df = pd.DataFrame(results)
            st.success("ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«æˆåŠŸã—ã¾ã—ãŸï¼")
            st.dataframe(df)
        else:
            st.warning("ç¾åœ¨ã¯ã‚µã‚¤ãƒˆå´ã§åˆ¶é™ãŒã‹ã‹ã£ã¦ã„ã¾ã™ã€‚15åˆ†ã»ã©ç©ºã‘ã¦ã‹ã‚‰ãŠè©¦ã—ãã ã•ã„ã€‚")
